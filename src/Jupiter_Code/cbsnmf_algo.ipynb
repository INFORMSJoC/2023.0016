{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b67ecef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "import os\n",
    "import data_readin\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import LP_solver as solver\n",
    "import xlsxwriter\n",
    "import itertools\n",
    "from numpy import random\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89d8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_formatter = \"{:.2f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bc92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perfrom hungarian algorithm for the 2D assignment problem \n",
    "# dist_matrix: input distance matrix\n",
    "# num_targets: number of targets\n",
    "# x_clean: solution for the 2D assignment problem\n",
    "# total_cost: total cost for the 2D assignment problem solution\n",
    "def assignment_problem(dist_matrix, num_targets):\n",
    "    row_ind, col_ind = linear_sum_assignment(dist_matrix)\n",
    "\n",
    "    total_cost = 0\n",
    "    x_clean = np.zeros((num_targets, num_targets))\n",
    "    for i in range(num_targets):\n",
    "        x_clean[i, col_ind[i]] = 1\n",
    "        total_cost = total_cost + dist_matrix[i][col_ind[i]]\n",
    "    return x_clean, total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e18582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for computing the total distance of a cluster under clique-based formulation\n",
    "def cost_calculator(cluster, dic):\n",
    "    cost = 0\n",
    "    q = len(cluster)\n",
    "    n = len(cluster[0])\n",
    "    reshaped_cluster = cluster.T.reshape(cluster.shape[0]*cluster.shape[1])\n",
    "\n",
    "    for i in range(q):\n",
    "        sub_cluster = np.array(np.where(reshaped_cluster==i))[0]\n",
    "        sub_cluster = np.array([sub_cluster[j]-j*q for j in range(n)])\n",
    "        for i in range(len(sub_cluster)): # number of views\n",
    "            for j in range(i+1, len(sub_cluster)):\n",
    "                cost += dic[(i+1, j+1)][int(sub_cluster[i])][int(sub_cluster[j])]\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e4d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the similarity matrix from each single source clustering results\n",
    "def get_indicator_matrix(input_source, num_targets, num_views):\n",
    "    indicator_matrix = np.zeros((num_targets*num_views, num_targets*num_views))\n",
    "\n",
    "    for i in range(num_targets*num_views):\n",
    "        indicator_set = input_source[:,int(i/num_targets)]\n",
    "        indicator_sectors = []\n",
    "        for j in range(num_views):\n",
    "            indicator_sector = np.zeros((num_targets)).tolist()\n",
    "            for k in range(num_targets):\n",
    "                if input_source[k,j] == indicator_set[i%num_targets]:\n",
    "                    indicator_sector[k] = 1\n",
    "                    indicator_sectors.append(indicator_sector)\n",
    "        indicator_sectors = np.array(list(itertools.chain(*indicator_sectors))).astype(int)\n",
    "        indicator_matrix[i,:] = indicator_sectors\n",
    "\n",
    "    indicator_matrix = indicator_matrix.astype(int)\n",
    "    return indicator_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3fad296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the normalized similarity matrix among all the single-source clustering results\n",
    "def get_norm_indicator_matrix(input_data_dict, num_targets, num_views, num_source):\n",
    "    input_keys = input_data_dict.keys()\n",
    "    indicator_matrices = []\n",
    "    for k in input_keys:\n",
    "        input_source = input_data_dict[k]\n",
    "        indicator_matrix = get_indicator_matrix(input_source, num_targets, num_views)\n",
    "        indicator_matrices.append(indicator_matrix)\n",
    "        \n",
    "    indicator_matrix = sum(indicator_matrices)\n",
    "    imax, jmax = indicator_matrix.shape\n",
    "    indicator_matrix = indicator_matrix.astype('float64')\n",
    "    for i in range(imax):\n",
    "        for j in range(jmax):\n",
    "            if indicator_matrix[i,j] != 0:\n",
    "                indicator_matrix[i,j] = round(float((indicator_matrix[i,j]/num_source)),3)\n",
    "            else:\n",
    "                indicator_matrix[i,j] = 0\n",
    "    return indicator_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa869299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the binary similarity matrix among all the single-source clustering results\n",
    "def get_scale_indicator_matrix(input_data_dict, num_targets, num_views):\n",
    "    input_keys = input_data_dict.keys()\n",
    "    indicator_matrices = []\n",
    "    for k in input_keys:\n",
    "        input_source = input_data_dict[k]\n",
    "        indicator_matrix = get_indicator_matrix(input_source, num_targets, num_views)\n",
    "        indicator_matrices.append(indicator_matrix)\n",
    "\n",
    "    indicator_matrices = sum(indicator_matrices)\n",
    "    imax, jmax = indicator_matrices.shape\n",
    "\n",
    "    for i in range(imax):\n",
    "        for j in range(jmax):\n",
    "            if indicator_matrices[i,j] > 0:\n",
    "                indicator_matrices[i,j] = 1\n",
    "    return indicator_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45074a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the density-based similarity matrix among all the single-source clustering results\n",
    "def density_indicator_matrix(input_data_dict, num_targets, num_views, num_source):\n",
    "    indicator_matrices = []\n",
    "    for i in range(5):\n",
    "        input_source = input_data_dict[i]\n",
    "        indicator_matrix = get_indicator_matrix(input_source, num_targets, num_views)\n",
    "        indicator_matrices.append(indicator_matrix)\n",
    "        \n",
    "    indicator_matrix = sum(indicator_matrices)\n",
    "    imax, jmax = indicator_matrix.shape\n",
    "    indicator_matrix = indicator_matrix.astype('float64')\n",
    "    \n",
    "    for i in range(imax):\n",
    "        for j in range(jmax):\n",
    "            if indicator_matrix[i,j] == 0:\n",
    "                indicator_matrix[i,j] = 0\n",
    "            else:\n",
    "                indicator_matrix[i,j] = round(float(1/(indicator_matrix[i,j]/num_source)),3)\n",
    "\n",
    "    return indicator_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c847236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the regularized matrix from the indicator matrix\n",
    "# connected_reg:   penality factor for targets clustered in the same group\n",
    "# unconeected_reg: penality factor for targets not clustered in the same group\n",
    "# connected_reg << unconeected_reg\n",
    "def get_reg_matrix(indicator_matrix, connected_reg, unconeected_reg):\n",
    "    matrix_dim = len(indicator_matrix)\n",
    "    reg_matrix = np.zeros((matrix_dim, matrix_dim))\n",
    "    for i in range(matrix_dim):\n",
    "        for j in range(matrix_dim):\n",
    "            if indicator_matrix[i,j] > 0:\n",
    "                reg_matrix[i,j] = connected_reg\n",
    "            else:\n",
    "                reg_matrix[i,j] = unconeected_reg\n",
    "    return reg_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "294596d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the scaled regularized matrix from the indicator matrix\n",
    "# the penality factor for connected two targets will be 0\n",
    "# the penality factor for unconnnected two targets will be \n",
    "# the maximum {2/indicator_matrices[i,j]} if ndicator_matrices[i,j] != 0\n",
    "def get_scale_reg_matrix(input_data_dict, num_targets, num_views):\n",
    "    input_keys = input_data_dict.keys()\n",
    "    indicator_matrices = []\n",
    "    for k in input_keys:\n",
    "        input_source = input_data_dict[k]\n",
    "        indicator_matrix = get_indicator_matrix(input_source, num_targets, num_views)\n",
    "        indicator_matrices.append(indicator_matrix)\n",
    "    indicator_matrices = sum(indicator_matrices)\n",
    "    imax, jmax = indicator_matrices.shape\n",
    "\n",
    "    scale_indicators = []\n",
    "    for i in range(imax):\n",
    "        scale_indi = []\n",
    "        for j in range(jmax):\n",
    "            if indicator_matrices[i,j] > 0:\n",
    "                scale_indi.append(round(float(1/indicator_matrices[i,j]),3))\n",
    "            else:\n",
    "                scale_indi.append(0)\n",
    "        scale_indicators.append(scale_indi)\n",
    "    scale_indicators = np.array(scale_indicators)\n",
    "\n",
    "    imax, jmax = scale_indicators.shape\n",
    "    penality_fac = np.amax(scale_indicators) * 2\n",
    "    for i in range(imax):\n",
    "        for j in range(jmax):\n",
    "            if scale_indicators[i,j] == 0:\n",
    "                scale_indicators[i,j] = penality_fac\n",
    "    return scale_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49712b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perfrom NMF\n",
    "# indicator_matrix: similarity matrix for factorization\n",
    "# reg_matrix: regularization matrix for cbsnmf\n",
    "# alpha:  coefficient for the regularization matrix\n",
    "# max_iter: maximum iteration for cbsnmf\n",
    "# num_targets: number of targets\n",
    "# num_views: number of sensors/stages\n",
    "def matrix_fact(indicator_matrix, reg_matrix, alpha, max_iter, num_targets, num_views):\n",
    "    u = random.randint(100, size=(num_targets*num_views,num_targets))\n",
    "    for i in range(max_iter):\n",
    "        numeroator = indicator_matrix@u\n",
    "        denominator = u@u.T@u + alpha*reg_matrix@u@u.T@reg_matrix@u\n",
    "        u = (numeroator/(denominator+1e-9))*u\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8545c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the concensus matrix from the output of the matrix factorization\n",
    "# u: output for the matrix factorization/Indicator matrix in paper\n",
    "# num_targets: number of targets\n",
    "# num_views: number of sensors/stages\n",
    "def get_concensus_matrix(u, num_targets, num_views):\n",
    "    output_res = np.zeros((num_targets, num_views))\n",
    "    for i in range(num_views):\n",
    "        row_ind, col_ind = linear_sum_assignment(-u[i*num_targets:(i+1)*num_targets,:])\n",
    "        for j in range(num_targets):\n",
    "            output_res[j,i] = col_ind[j]\n",
    "    return output_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d09ce9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00 4.00 0.00]\n",
      " [3.00 1.00 1.00]\n",
      " [2.00 2.00 3.00]\n",
      " [1.00 3.00 2.00]\n",
      " [4.00 0.00 4.00]]\n"
     ]
    }
   ],
   "source": [
    "# Testing for CBSNMF Algorithms\n",
    "# User may alter dimensions for CBSNMF methods:\n",
    "#    num_views -> number of sensors/stages\n",
    "#    num_targets -> number of targets\n",
    "#    num_source -> number of data sources\n",
    "#    input_data_dict -> outputs from single-source methods\n",
    "#    connected_reg -> penality factor for targets clustered in the same group\n",
    "#    unconeected_reg -> penality factor for targets not clustered in the same group\n",
    "#    alpha -> coefficient for the regularization matrix\n",
    "#    max_iter -> maximum iteration for cbsnmf\n",
    "# Please choose to un/comment the print statement correspond to indicator and regularization matrix\n",
    "# users may want to review.\n",
    "\n",
    "def main():\n",
    "    num_targets = 5\n",
    "    num_views = 3\n",
    "    num_source = 5\n",
    "    input_data_dict = {0: np.array([[0, 3, 4],\n",
    "                           [1, 1, 1],\n",
    "                           [2, 2, 2],\n",
    "                           [3, 0, 3],\n",
    "                           [4, 4, 0]]),\n",
    "                       1: np.array([[0, 2, 4],\n",
    "                           [1, 0, 1],\n",
    "                           [2, 1, 3],\n",
    "                           [3, 4, 2],\n",
    "                           [4, 3, 0]]),\n",
    "                       2: np.array([[0, 2, 2],\n",
    "                           [1, 3, 3],\n",
    "                           [2, 0, 1],\n",
    "                           [3, 4, 4],\n",
    "                           [4, 1, 0]]),\n",
    "                       3: np.array([[0, 1, 0],\n",
    "                           [1, 2, 1],\n",
    "                           [2, 4, 3],\n",
    "                           [3, 3, 2],\n",
    "                           [4, 0, 4]]),\n",
    "                       4: np.array([[0, 1, 0],\n",
    "                           [1, 0, 2],\n",
    "                           [2, 4, 3],\n",
    "                           [3, 2, 4],\n",
    "                           [4, 3, 1]])}\n",
    "    \n",
    "    connected_reg = 0.1\n",
    "    unconeected_reg = 10\n",
    "    alpha = 10\n",
    "    max_iter = 1000\n",
    "    \n",
    "    # choose type of the similarity matrix\n",
    "    indicator_matrix = get_norm_indicator_matrix(input_data_dict, num_targets, num_views, num_source)\n",
    "    #indicator_matrix = get_scale_indicator_matrix(input_data_dict, num_targets, num_views)\n",
    "    #indicator_matrix = density_indicator_matrix(input_data_dict, num_targets, num_views, num_source)\n",
    "    \n",
    "    # choose type of the regularization matrix\n",
    "    g = get_reg_matrix(indicator_matrix, connected_reg, unconeected_reg)\n",
    "    #g = get_scale_reg_matrix(input_data_dict, num_targets, num_views)\n",
    "    \n",
    "    u = matrix_fact(indicator_matrix, g, alpha, max_iter, num_targets, num_views)\n",
    "    nmf_results = get_concensus_matrix(u, num_targets, num_views)\n",
    "    print(nmf_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
